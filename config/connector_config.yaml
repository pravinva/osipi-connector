# OSI PI Lakeflow Connector Configuration
#
# This configuration file defines how the connector extracts data from
# OSI PI Systems and loads into Databricks Unity Catalog

# PI Web API Configuration
pi_web_api:
  # PI Web API endpoint URL (required)
  url: "https://pi-server.your-company.com/piwebapi"

  # Authentication type: basic, kerberos, or oauth
  auth_type: "basic"

  # For basic auth (credentials stored in Databricks Secrets)
  # Reference: dbutils.secrets.get("pi-connector", "pi_username")
  secrets_scope: "pi-connector"
  username_key: "pi_username"
  password_key: "pi_password"

  # For OAuth (token stored in Databricks Secrets)
  # oauth_token_key: "oauth_token"

  # For Kerberos (no credentials needed if cluster is kerberized)
  # mutual_authentication: "OPTIONAL"

# Unity Catalog Configuration
unity_catalog:
  # Catalog name (required)
  catalog: "main"

  # Schema for PI data (required)
  schema: "bronze"

  # Table names
  tables:
    timeseries: "pi_timeseries"
    asset_hierarchy: "pi_asset_hierarchy"
    event_frames: "pi_event_frames"

  # Checkpoint schema for watermarking
  checkpoint_schema: "checkpoints"
  checkpoint_table: "pi_watermarks"

# Data Extraction Configuration
extraction:
  # Tags to extract
  # Option 1: List specific tag WebIds
  tags:
    - "F1DPQ0X9jJTUG0K-6g-2EFN0wQQUktTU9OQVwtU0VSVkVSXFRBRzAwMQ"
    - "F1DPQ0X9jJTUG0K-6g-2EFN0wQQUktTU9OQVwtU0VSVkVSXFRBRzAwMg"

  # Option 2: Extract all tags from data server (set tags to null)
  # tags: null
  # extract_all_tags: true

  # Option 3: Filter tags by name pattern
  # tag_name_filter: "PLANT-A.*"

  # Batch size for extraction (tags per HTTP request)
  # Recommended: 100 (optimal for PI Web API batch controller)
  batch_size: 100

  # Time range for extraction
  # "incremental" uses checkpoint, "full" extracts from start_date
  mode: "incremental"

  # For full load, specify start date
  start_date: "2024-01-01T00:00:00Z"

  # Maximum records per tag per request
  max_count_per_tag: 10000

  # Enable automatic paging for large time ranges
  enable_paging: true

# PI Asset Framework Configuration
asset_framework:
  # Enable AF hierarchy extraction
  enabled: true

  # AF Database WebId (required if enabled)
  database_webid: "F1EM...AF-DATABASE-WEBID"

  # Maximum depth to traverse (default: unlimited)
  max_depth: null

  # Element template filters (extract only these templates)
  # Leave empty to extract all templates
  template_filters: []

  # Extract attributes
  extract_attributes: true

# Event Frames Configuration
event_frames:
  # Enable Event Frame extraction
  enabled: true

  # AF Database WebId (same as AF hierarchy, required if enabled)
  database_webid: "F1EM...AF-DATABASE-WEBID"

  # Event Frame template filters (extract only these templates)
  # Leave empty to extract all templates
  template_filters:
    - "Batch"
    - "Maintenance Event"
    - "Alarm"

  # Search by time range (uses extraction.start_date and current time)
  time_based_search: true

  # Maximum event frames to extract per run
  max_events: 10000

# Performance Tuning
performance:
  # Number of Spark partitions for processing
  # Set to cluster cores * 2-3
  spark_partitions: 200

  # Enable Delta table optimizations
  optimize_after_write: true

  # Z-ORDER columns for timeseries table
  zorder_columns:
    - "tag_webid"
    - "timestamp"

  # Partition strategy for timeseries table
  partition_by:
    - "partition_date"  # Derived from timestamp

# Data Quality
data_quality:
  # Preserve PI quality flags
  preserve_quality_flags: true

  # Filter out bad quality data during extraction
  filter_bad_quality: false

  # Add ingestion metadata
  add_ingestion_timestamp: true

  # Validate data ranges (detect outliers)
  validate_ranges: false

# Scheduling
schedule:
  # Cron expression for job scheduling
  # Default: Hourly (0 0 * * * ?)
  # Daily: 0 0 0 * * ?
  # Every 15 min: 0 0/15 * * * ?
  cron_expression: "0 0 * * * ?"

  # Timezone
  timezone: "UTC"

  # Pause on errors
  pause_on_failure: false

# Monitoring and Alerting
monitoring:
  # Enable logging
  log_level: "INFO"

  # Send alerts on failures
  alert_on_failure: true

  # Email recipients for alerts
  alert_emails:
    - "data-team@your-company.com"

  # Slack webhook for notifications
  # slack_webhook_url: "https://hooks.slack.com/services/..."

  # Track extraction metrics
  track_metrics: true

  # Metrics table
  metrics_catalog: "main"
  metrics_schema: "monitoring"
  metrics_table: "pi_connector_metrics"

# Retry Configuration
retry:
  # Maximum retries for transient failures
  max_retries: 3

  # Backoff strategy: fixed, exponential
  backoff_strategy: "exponential"

  # Initial retry delay (seconds)
  initial_delay: 5

  # Maximum retry delay (seconds)
  max_delay: 300

  # Retry on these HTTP status codes
  retry_status_codes:
    - 429  # Too Many Requests
    - 500  # Internal Server Error
    - 502  # Bad Gateway
    - 503  # Service Unavailable
    - 504  # Gateway Timeout

# Development/Testing
development:
  # Use mock PI server for testing
  use_mock_server: false

  # Mock server URL (if enabled)
  mock_server_url: "http://localhost:8000"

  # Dry run mode (validate config without writing data)
  dry_run: false

  # Sample mode (extract only first N tags for testing)
  sample_mode: false
  sample_size: 10
