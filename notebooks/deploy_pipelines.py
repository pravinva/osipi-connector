# Databricks notebook source
# MAGIC %md
# MAGIC # Deploy OSI PI Pipelines via Databricks Asset Bundles
# MAGIC
# MAGIC This notebook deploys the DLT pipelines generated by `generate_pipelines_from_mock_api.py`
# MAGIC
# MAGIC **Run this notebook after generating pipeline configurations**
# MAGIC
# MAGIC Steps:
# MAGIC 1. Validate DAB configuration
# MAGIC 2. Deploy pipelines to Databricks
# MAGIC 3. Show deployment summary

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configuration

# COMMAND ----------

import os
import subprocess
import json

# Deployment target (dev or prod)
DAB_TARGET = "dev"

# Project root (adjust if needed)
PROJECT_ROOT = "/Workspace/Repos/production/osipi-connector"

print(f"Deployment Target: {DAB_TARGET}")
print(f"Project Root: {PROJECT_ROOT}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Important: CLI Commands Cannot Run in Notebooks
# MAGIC
# MAGIC Databricks notebooks don't support `subprocess` calls to the CLI.
# MAGIC
# MAGIC **Instead, use one of these approaches:**
# MAGIC
# MAGIC ### Option 1: Local Terminal (Recommended)
# MAGIC ```bash
# MAGIC cd /path/to/osipi-connector
# MAGIC databricks bundle validate -t dev
# MAGIC databricks bundle deploy -t dev
# MAGIC ```
# MAGIC
# MAGIC ### Option 2: Databricks REST API (Advanced)
# MAGIC Use the Databricks SDK to deploy resources programmatically.
# MAGIC See cells below for examples.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Deploy Using Databricks SDK (Alternative)

# COMMAND ----------

print("=" * 80)
print("Deployment Instructions")
print("=" * 80)
print("\nTo deploy the generated DAB configuration:")
print("\n1. Open a terminal with Databricks CLI installed and configured")
print(f"2. Navigate to: {PROJECT_ROOT}")
print(f"3. Validate: databricks bundle validate -t {DAB_TARGET}")
print(f"4. Deploy: databricks bundle deploy -t {DAB_TARGET}")
print("\nOR if using a local clone:")
print("1. Clone repo: git clone <your-repo-url>")
print("2. cd osipi-connector")
print(f"3. databricks bundle deploy -t {DAB_TARGET}")
print("\n" + "=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## View Deployed Resources (SDK Method)

# COMMAND ----------

from databricks.sdk import WorkspaceClient

w = WorkspaceClient()

print("=" * 80)
print("Current DLT Pipelines (OSI PI)")
print("=" * 80)

try:
    # List all pipelines with "osipi" in the name
    pipelines = [p for p in w.pipelines.list_pipelines() if p.name and "osipi" in p.name.lower()]

    if pipelines:
        print(f"\nFound {len(pipelines)} OSI PI pipeline(s):")
        for p in pipelines:
            print(f"\n  Pipeline: {p.name}")
            print(f"  ID: {p.pipeline_id}")
            print(f"  State: {p.state}")
            print(f"  Target: {p.spec.target if p.spec else 'N/A'}")
    else:
        print("\nNo OSI PI pipelines found.")
        print("Deploy the DAB bundle first using the CLI commands above.")
except Exception as e:
    print(f"\nError listing pipelines: {e}")

print("\n" + "=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Next Steps

# COMMAND ----------

print("=" * 80)
print("After Deployment")
print("=" * 80)
print("\n1. View Pipelines:")
print("   - Go to Workflows → Delta Live Tables")
print("   - Look for pipelines named 'osipi_demo_ingestion_group_*'")
print("\n2. View Jobs (batch mode only):")
print("   - Go to Workflows → Jobs")
print("   - Look for jobs named 'osipi_demo_job_group_*'")
print("\n3. Monitor Data:")
print("   - Go to Data Explorer → osipi.bronze")
print("   - Check tables: pi_timeseries, pi_af_hierarchy, pi_event_frames")
print("\n4. Trigger Manual Run (optional):")
print("   - See 'Optional: Trigger Pipeline Runs' cell below")
print("\n" + "=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Optional: Trigger Pipeline Runs (Batch Mode)

# COMMAND ----------

# Uncomment to manually trigger all pipeline runs after deployment

# import time
# from databricks.sdk import WorkspaceClient
#
# w = WorkspaceClient()
#
# print("Triggering pipeline runs...")
#
# # Get all pipelines with name pattern
# pipelines = [p for p in w.pipelines.list_pipelines() if p.name and "osipi" in p.name.lower()]
#
# print(f"Found {len(pipelines)} pipelines to trigger")
#
# for pipeline in pipelines:
#     try:
#         update = w.pipelines.start_update(pipeline_id=pipeline.pipeline_id)
#         print(f"✓ Triggered: {pipeline.name} (update_id: {update.update_id})")
#     except Exception as e:
#         print(f"✗ Failed to trigger {pipeline.name}: {e}")
#     time.sleep(1)  # Rate limiting
#
# print("\nAll pipelines triggered. Monitor progress in DLT UI.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Troubleshooting

# COMMAND ----------

# MAGIC %md
# MAGIC ### Common Issues:
# MAGIC
# MAGIC **1. "databricks: command not found"**
# MAGIC - Install Databricks CLI: `pip install databricks-cli`
# MAGIC - Or use `%pip install databricks-cli` in a cell above
# MAGIC
# MAGIC **2. "Authentication failed"**
# MAGIC - Ensure you're logged in: `databricks auth login --host <workspace-url>`
# MAGIC - Check your `.databrickscfg` file
# MAGIC
# MAGIC **3. "Project root not found"**
# MAGIC - Update `PROJECT_ROOT` variable to match your repo location
# MAGIC - Common paths:
# MAGIC   - `/Workspace/Repos/<username>/osipi-connector`
# MAGIC   - `/Workspace/Users/<email>/osipi-connector`
# MAGIC
# MAGIC **4. "Validation failed - notebook not found"**
# MAGIC - Make sure `src/notebooks/pi_ingestion_pipeline.py` exists
# MAGIC - Check that notebook paths in `deployment/resources/pipelines.yml` are correct
# MAGIC
# MAGIC **5. "Resource already exists"**
# MAGIC - This is normal for updates - DAB will update existing resources
# MAGIC - To force recreation, delete resources in UI first
