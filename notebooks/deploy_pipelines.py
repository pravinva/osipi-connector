# Databricks notebook source
# MAGIC %md
# MAGIC # Deploy OSI PI Pipelines via Databricks Asset Bundles
# MAGIC
# MAGIC This notebook deploys the DLT pipelines generated by `generate_pipelines_from_mock_api.py`
# MAGIC
# MAGIC **Run this notebook after generating pipeline configurations**
# MAGIC
# MAGIC Steps:
# MAGIC 1. Validate DAB configuration
# MAGIC 2. Deploy pipelines to Databricks
# MAGIC 3. Show deployment summary

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configuration

# COMMAND ----------

import os
import subprocess
import json

# Deployment target (dev or prod)
DAB_TARGET = "dev"

# Project root (adjust if needed)
PROJECT_ROOT = "/Workspace/Repos/production/osipi-connector"

print(f"Deployment Target: {DAB_TARGET}")
print(f"Project Root: {PROJECT_ROOT}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 1: Validate DAB Configuration

# COMMAND ----------

print("=" * 80)
print("Validating Databricks Asset Bundle Configuration...")
print("=" * 80)

# Change to project directory and validate
result = subprocess.run(
    ["databricks", "bundle", "validate", "-t", DAB_TARGET],
    cwd=PROJECT_ROOT,
    capture_output=True,
    text=True
)

if result.returncode == 0:
    print("✓ Validation PASSED")
    print("\nOutput:")
    print(result.stdout)
else:
    print("✗ Validation FAILED")
    print("\nError:")
    print(result.stderr)
    raise Exception("DAB validation failed. Please fix configuration errors before deploying.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 2: Deploy Pipelines

# COMMAND ----------

print("=" * 80)
print(f"Deploying to {DAB_TARGET.upper()} environment...")
print("=" * 80)

# Deploy the bundle
result = subprocess.run(
    ["databricks", "bundle", "deploy", "-t", DAB_TARGET],
    cwd=PROJECT_ROOT,
    capture_output=True,
    text=True
)

if result.returncode == 0:
    print("✓ Deployment SUCCEEDED")
    print("\nOutput:")
    print(result.stdout)
else:
    print("✗ Deployment FAILED")
    print("\nError:")
    print(result.stderr)
    raise Exception("DAB deployment failed. Check error messages above.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Step 3: Deployment Summary

# COMMAND ----------

print("=" * 80)
print("Deployment Complete!")
print("=" * 80)

# Get deployed resources
try:
    result = subprocess.run(
        ["databricks", "bundle", "resources", "-t", DAB_TARGET],
        cwd=PROJECT_ROOT,
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        print("\nDeployed Resources:")
        print(result.stdout)
    else:
        print("\nCouldn't fetch resource list (this is okay, deployment succeeded)")
except Exception as e:
    print(f"\nNote: Couldn't fetch resource list: {e}")

print("\n" + "=" * 80)
print("Next Steps:")
print("=" * 80)
print("1. Go to Workflows → Delta Live Tables to see your pipelines")
print("2. Go to Workflows → Jobs to see scheduled jobs (batch mode only)")
print("3. Monitor pipeline runs in the DLT UI")
print("4. Check ingested data in Data Explorer → osipi.bronze")
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Optional: Trigger Pipeline Runs (Batch Mode)

# COMMAND ----------

# Uncomment to manually trigger all pipeline runs after deployment

# import time
# from databricks.sdk import WorkspaceClient
#
# w = WorkspaceClient()
#
# print("Triggering pipeline runs...")
#
# # Get all pipelines with name pattern
# pipelines = [p for p in w.pipelines.list_pipelines() if p.name and "osipi" in p.name.lower()]
#
# print(f"Found {len(pipelines)} pipelines to trigger")
#
# for pipeline in pipelines:
#     try:
#         update = w.pipelines.start_update(pipeline_id=pipeline.pipeline_id)
#         print(f"✓ Triggered: {pipeline.name} (update_id: {update.update_id})")
#     except Exception as e:
#         print(f"✗ Failed to trigger {pipeline.name}: {e}")
#     time.sleep(1)  # Rate limiting
#
# print("\nAll pipelines triggered. Monitor progress in DLT UI.")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Troubleshooting

# COMMAND ----------

# MAGIC %md
# MAGIC ### Common Issues:
# MAGIC
# MAGIC **1. "databricks: command not found"**
# MAGIC - Install Databricks CLI: `pip install databricks-cli`
# MAGIC - Or use `%pip install databricks-cli` in a cell above
# MAGIC
# MAGIC **2. "Authentication failed"**
# MAGIC - Ensure you're logged in: `databricks auth login --host <workspace-url>`
# MAGIC - Check your `.databrickscfg` file
# MAGIC
# MAGIC **3. "Project root not found"**
# MAGIC - Update `PROJECT_ROOT` variable to match your repo location
# MAGIC - Common paths:
# MAGIC   - `/Workspace/Repos/<username>/osipi-connector`
# MAGIC   - `/Workspace/Users/<email>/osipi-connector`
# MAGIC
# MAGIC **4. "Validation failed - notebook not found"**
# MAGIC - Make sure `src/notebooks/pi_ingestion_pipeline.py` exists
# MAGIC - Check that notebook paths in `deployment/resources/pipelines.yml` are correct
# MAGIC
# MAGIC **5. "Resource already exists"**
# MAGIC - This is normal for updates - DAB will update existing resources
# MAGIC - To force recreation, delete resources in UI first
