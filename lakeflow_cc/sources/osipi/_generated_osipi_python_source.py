# ==============================================================================
# Merged Lakeflow Source: osipi
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta, timezone
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Optional,
    Tuple,
)

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/osipi/osipi.py
    ########################################################

    def _utcnow() -> datetime:
        return datetime.now(timezone.utc)


    def _isoformat_z(dt: datetime) -> str:
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


    def _parse_ts(value: str) -> datetime:
        return datetime.fromisoformat(value.replace("Z", "+00:00"))


    class LakeflowConnect:
        TABLE_DATASERVERS = "pi_dataservers"
        TABLE_POINTS = "pi_points"
        TABLE_TIMESERIES = "pi_timeseries"
        TABLE_AF_HIERARCHY = "pi_af_hierarchy"
        TABLE_EVENT_FRAMES = "pi_event_frames"

        def __init__(self, options: Dict[str, str]) -> None:
            self.options = options
            self.base_url = (options.get("pi_base_url") or options.get("pi_web_api_url") or "").rstrip("/")
            if not self.base_url:
                raise ValueError("Missing required option: pi_base_url (or pi_web_api_url)")

            self.session = requests.Session()
            self.session.headers.update({"Accept": "application/json"})
            self._auth_resolved = False

        def list_tables(self) -> List[str]:
            return [
                self.TABLE_DATASERVERS,
                self.TABLE_POINTS,
                self.TABLE_TIMESERIES,
                self.TABLE_AF_HIERARCHY,
                self.TABLE_EVENT_FRAMES,
            ]

        def get_table_schema(self, table_name: str, table_options: Dict[str, str]) -> StructType:
            if table_name == self.TABLE_DATASERVERS:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                ])

            if table_name == self.TABLE_POINTS:
                return StructType([
                    StructField("webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("descriptor", StringType(), True),
                    StructField("engineering_units", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("dataserver_webid", StringType(), True),
                ])

            if table_name == self.TABLE_TIMESERIES:
                return StructType([
                    StructField("tag_webid", StringType(), False),
                    StructField("timestamp", TimestampType(), False),
                    StructField("value", DoubleType(), True),
                    StructField("good", BooleanType(), True),
                    StructField("units", StringType(), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_AF_HIERARCHY:
                return StructType([
                    StructField("element_webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("template_name", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("path", StringType(), True),
                    StructField("parent_webid", StringType(), True),
                    StructField("depth", IntegerType(), True),
                    StructField("category_names", ArrayType(StringType()), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            if table_name == self.TABLE_EVENT_FRAMES:
                return StructType([
                    StructField("event_frame_webid", StringType(), False),
                    StructField("name", StringType(), True),
                    StructField("template_name", StringType(), True),
                    StructField("start_time", TimestampType(), True),
                    StructField("end_time", TimestampType(), True),
                    StructField("primary_referenced_element_webid", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("category_names", ArrayType(StringType()), True),
                    StructField("attributes", MapType(StringType(), StringType()), True),
                    StructField("ingestion_timestamp", TimestampType(), False),
                ])

            raise ValueError(f"Unknown table: {table_name}")

        def read_table_metadata(self, table_name: str, table_options: Dict[str, str]) -> Dict:
            if table_name == self.TABLE_DATASERVERS:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_POINTS:
                return {"primary_keys": ["webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_TIMESERIES:
                return {"primary_keys": ["tag_webid", "timestamp"], "cursor_field": "timestamp", "ingestion_type": "append"}
            if table_name == self.TABLE_AF_HIERARCHY:
                return {"primary_keys": ["element_webid"], "cursor_field": None, "ingestion_type": "snapshot"}
            if table_name == self.TABLE_EVENT_FRAMES:
                return {"primary_keys": ["event_frame_webid", "start_time"], "cursor_field": "start_time", "ingestion_type": "append"}
            raise ValueError(f"Unknown table: {table_name}")

        def read_table(self, table_name: str, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            self._ensure_auth()

            if table_name == self.TABLE_DATASERVERS:
                return iter(self._read_dataservers()), {"offset": "done"}
            if table_name == self.TABLE_POINTS:
                return iter(self._read_points(table_options)), {"offset": "done"}
            if table_name == self.TABLE_TIMESERIES:
                return self._read_timeseries(start_offset, table_options)
            if table_name == self.TABLE_AF_HIERARCHY:
                return iter(self._read_af_hierarchy()), {"offset": "done"}
            if table_name == self.TABLE_EVENT_FRAMES:
                return self._read_event_frames(start_offset, table_options)

            raise ValueError(f"Unknown table: {table_name}")

        def _ensure_auth(self) -> None:
            if self._auth_resolved:
                return

            access_token = self.options.get("access_token")
            if access_token:
                self.session.headers.update({"Authorization": f"Bearer {access_token}"})
                self._auth_resolved = True
                return

            workspace_host = (self.options.get("workspace_host") or "").rstrip("/")
            client_id = self.options.get("client_id")
            client_secret = self.options.get("client_secret")
            if workspace_host and client_id and client_secret:
                if not workspace_host.startswith("http://") and not workspace_host.startswith("https://"):
                    workspace_host = "https://" + workspace_host

                token_url = f"{workspace_host}/oidc/v1/token"
                resp = requests.post(
                    token_url,
                    data={"grant_type": "client_credentials", "scope": "all-apis"},
                    auth=(client_id, client_secret),
                    headers={"Content-Type": "application/x-www-form-urlencoded"},
                    timeout=30,
                )
                resp.raise_for_status()
                token = resp.json().get("access_token")
                if not token:
                    raise RuntimeError("OIDC token endpoint did not return access_token")
                self.session.headers.update({"Authorization": f"Bearer {token}"})
                self._auth_resolved = True
                return

            username = self.options.get("username")
            password = self.options.get("password")
            if username and password:
                self.session.auth = (username, password)
                self._auth_resolved = True
                return

            self._auth_resolved = True

        def _get_json(self, path: str, params: Optional[Dict] = None) -> dict:
            url = f"{self.base_url}{path}"
            r = self.session.get(url, params=params, timeout=60)
            r.raise_for_status()
            return r.json()

        def _post_json(self, path: str, payload: dict) -> dict:
            url = f"{self.base_url}{path}"
            r = self.session.post(url, json=payload, timeout=120)
            r.raise_for_status()
            return r.json()

        def _read_dataservers(self) -> List[dict]:
            data = self._get_json("/piwebapi/dataservers")
            items = data.get("Items", [])
            return [{"webid": i.get("WebId"), "name": i.get("Name")} for i in items if i.get("WebId")]

        def _read_points(self, table_options: Dict[str, str]) -> List[dict]:
            dataservers = self._read_dataservers()
            if not dataservers:
                return []
            server_webid = table_options.get("dataserver_webid") or dataservers[0]["webid"]

            params: Dict[str, str] = {"maxCount": str(table_options.get("maxCount", 10000))}
            name_filter = table_options.get("nameFilter")
            if name_filter:
                params["nameFilter"] = name_filter

            data = self._get_json(f"/piwebapi/dataservers/{server_webid}/points", params=params)
            items = data.get("Items", [])
            out: List[dict] = []
            for p in items:
                out.append({
                    "webid": p.get("WebId"),
                    "name": p.get("Name"),
                    "descriptor": p.get("Descriptor", ""),
                    "engineering_units": p.get("EngineeringUnits", ""),
                    "path": p.get("Path", ""),
                    "dataserver_webid": server_webid,
                })
            return [r for r in out if r.get("webid")]

        def _read_timeseries(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            tag_webids_csv = table_options.get("tag_webids") or self.options.get("tag_webids") or ""
            tag_webids = [t.strip() for t in tag_webids_csv.split(",") if t.strip()]
            if not tag_webids:
                pts = self._read_points(table_options)
                tag_webids = [p["webid"] for p in pts[: int(table_options.get("default_tags", 50))]]

            now = _utcnow()

            start = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start = _parse_ts(off)
                    except Exception:
                        start = None
            if start is None:
                lookback_minutes = int(table_options.get("lookback_minutes", 60))
                start = now - timedelta(minutes=lookback_minutes)

            start_str = _isoformat_z(start)
            end_str = _isoformat_z(now)
            max_count = int(table_options.get("maxCount", 1000))

            requests_list = [
                {
                    "Method": "GET",
                    "Resource": f"/piwebapi/streams/{webid}/recorded",
                    "Parameters": {"startTime": start_str, "endTime": end_str, "maxCount": str(max_count)},
                }
                for webid in tag_webids
            ]

            batch = self._post_json("/piwebapi/batch", {"Requests": requests_list})
            responses = batch.get("Responses", [])
            ingest_ts = _utcnow()

            def iterator() -> Iterator[dict]:
                for idx, resp in enumerate(responses):
                    if resp.get("Status") != 200:
                        continue
                    webid = tag_webids[idx] if idx < len(tag_webids) else None
                    content = resp.get("Content", {})
                    for item in content.get("Items", []):
                        ts = item.get("Timestamp")
                        if not ts or not webid:
                            continue
                        yield {
                            "tag_webid": webid,
                            "timestamp": _parse_ts(ts),
                            "value": float(item.get("Value")) if item.get("Value") is not None else None,
                            "good": bool(item.get("Good", True)),
                            "units": item.get("UnitsAbbreviation", ""),
                            "ingestion_timestamp": ingest_ts,
                        }

            return iterator(), {"offset": end_str}

        def _read_af_hierarchy(self) -> List[dict]:
            db_list = self._post_json("/piwebapi/assetdatabases/list", {})
            dbs = db_list.get("Items", [])
            if not dbs:
                return []

            out: List[dict] = []
            ingest_ts = _utcnow()

            def walk(elements: List[dict], parent_webid: str, depth: int):
                for e in elements:
                    webid = e.get("WebId")
                    if not webid:
                        continue
                    out.append({
                        "element_webid": webid,
                        "name": e.get("Name", ""),
                        "template_name": e.get("TemplateName", ""),
                        "description": e.get("Description", ""),
                        "path": e.get("Path", ""),
                        "parent_webid": parent_webid or "",
                        "depth": depth,
                        "category_names": e.get("CategoryNames") or [],
                        "ingestion_timestamp": ingest_ts,
                    })
                    children = e.get("Elements") or []
                    if children:
                        walk(children, webid, depth + 1)

            for db in dbs:
                db_webid = db.get("WebId")
                if not db_webid:
                    continue
                roots = self._post_json("/piwebapi/assetdatabases/elements", {"db_webid": db_webid}).get("Items", [])
                walk(roots, parent_webid="", depth=0)

            return out

        def _read_event_frames(self, start_offset: dict, table_options: Dict[str, str]) -> Tuple[Iterator[dict], dict]:
            now = _utcnow()
            start = None
            if start_offset and isinstance(start_offset, dict):
                off = start_offset.get("offset")
                if isinstance(off, str) and off:
                    try:
                        start = _parse_ts(off)
                    except Exception:
                        start = None
            if start is None:
                lookback_days = int(table_options.get("lookback_days", 30))
                start = now - timedelta(days=lookback_days)

            start_str = _isoformat_z(start)
            end_str = _isoformat_z(now)

            db_list = self._post_json("/piwebapi/assetdatabases/list", {})
            dbs = db_list.get("Items", [])
            if not dbs:
                return iter(()), {"offset": end_str}

            all_events: List[dict] = []
            for db in dbs:
                db_webid = db.get("WebId")
                if not db_webid:
                    continue
                resp = self._post_json(
                    "/piwebapi/assetdatabases/eventframes",
                    {"db_webid": db_webid, "startTime": start_str, "endTime": end_str, "searchMode": "Overlapped"},
                )
                all_events.extend(resp.get("Items", []))

            ingest_ts = _utcnow()

            def iterator() -> Iterator[dict]:
                for ef in all_events:
                    webid = ef.get("WebId")
                    if not webid:
                        continue
                    raw_attrs = ef.get("Attributes") or {}
                    attrs = {str(k): ("" if v is None else str(v)) for k, v in raw_attrs.items()}
                    yield {
                        "event_frame_webid": webid,
                        "name": ef.get("Name", ""),
                        "template_name": ef.get("TemplateName", ""),
                        "start_time": _parse_ts(ef.get("StartTime")) if ef.get("StartTime") else None,
                        "end_time": _parse_ts(ef.get("EndTime")) if ef.get("EndTime") else None,
                        "primary_referenced_element_webid": ef.get("PrimaryReferencedElementWebId"),
                        "description": ef.get("Description", ""),
                        "category_names": ef.get("CategoryNames") or [],
                        "attributes": attrs,
                        "ingestion_timestamp": ingest_ts,
                    }

            return iterator(), {"offset": end_str}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
