# OSI PI Connector - Operator Guide

## Quick Start - 3 Simple Steps

### Step 1: Configure Pipeline Settings

Open `notebooks/generate_pipelines_from_mock_api.py` in Databricks and edit the configuration:

```python
# Line 38: Choose ingestion mode
INGESTION_MODE = "batch"      # Options: "batch" or "streaming"

# Line 42: How many tags per pipeline?
TAGS_PER_PIPELINE = 100       # Adjust for load balancing

# Line 48: Batch schedule (only for batch mode)
DEFAULT_BATCH_SCHEDULE = "0 */30 * * * ?"  # Every 30 minutes

# Line 30-32: Target location
TARGET_CATALOG = "osipi"
TARGET_SCHEMA = "bronze"
```

### Step 2: Generate Pipeline Configurations

In Databricks workspace:
1. Open `notebooks/generate_pipelines_from_mock_api.py`
2. Click **Run All**
3. Wait for completion (takes ~30 seconds)

The notebook will:
- Discover all tags from PI Web API
- Create load-balanced pipeline groups
- Generate DAB YAML configuration files

### Step 3: Deploy Pipelines

In Databricks workspace:
1. Open `notebooks/deploy_pipelines.py`
2. Update `PROJECT_ROOT` if needed (line 18)
3. Click **Run All**

This will:
- Validate DAB configuration
- Deploy DLT pipelines to your workspace
- Create scheduled jobs (batch mode) or continuous pipelines (streaming mode)

---

## Ingestion Modes Explained

### Batch Mode (Scheduled)
```python
INGESTION_MODE = "batch"
```
- Runs on a schedule (e.g., every 30 minutes)
- Cost-effective (only pay when running)
- Good for: Periodic data refresh, non-urgent data

**How it works:**
- Scheduled jobs trigger pipeline runs
- Each run ingests data since last checkpoint
- Pipelines stop after processing available data

### Streaming Mode (Real-time)
```python
INGESTION_MODE = "streaming"
```
- Runs continuously 24/7
- Real-time data ingestion
- Good for: Low-latency requirements, real-time dashboards

**How it works:**
- Pipelines run continuously
- No scheduled jobs needed
- Auto-scales based on data volume

---

## Configuration Options

### Load Balancing
```python
TAGS_PER_PIPELINE = 100  # Tags per pipeline
```
- **More tags per pipeline**: Fewer pipelines, simpler management
- **Fewer tags per pipeline**: More parallelism, faster ingestion

**Example:** 1,000 tags with `TAGS_PER_PIPELINE = 100` → 10 pipelines

### Batch Schedules
```python
SCHEDULE_15MIN = "0 */15 * * * ?"   # Every 15 minutes
SCHEDULE_30MIN = "0 */30 * * * ?"   # Every 30 minutes
SCHEDULE_HOURLY = "0 0 * * * ?"     # Every hour
```

Format: Quartz cron expression
- `0 */15 * * * ?` = minute 0, every 15 minutes, any hour/day
- `0 0 * * * ?` = minute 0, hour 0, every day

### API Configuration
```python
MOCK_API_URL = "https://osipi-webserver-xxx.aws.databricksapps.com"
CONNECTION_NAME = "mock_pi_connection"
```

Update these to point to your actual PI Web API server.

---

## Monitoring

### View Pipelines
**Databricks UI → Workflows → Delta Live Tables**
- See all deployed pipelines
- Monitor pipeline health and status
- View data lineage

### View Jobs (Batch Mode Only)
**Databricks UI → Workflows → Jobs**
- See scheduled job runs
- Check run history and logs
- Trigger manual runs

### View Ingested Data
**Databricks UI → Data → Catalog**
- Navigate to `osipi.bronze`
- Tables:
  - `pi_timeseries` - Time-series sensor data
  - `pi_af_hierarchy` - Asset Framework hierarchy
  - `pi_event_frames` - Event frames

---

## Switching Between Modes

To switch from batch to streaming (or vice versa):

1. Open `notebooks/generate_pipelines_from_mock_api.py`
2. Change `INGESTION_MODE = "batch"` to `INGESTION_MODE = "streaming"` (or vice versa)
3. Run the notebook (Run All)
4. Open `notebooks/deploy_pipelines.py`
5. Run the notebook (Run All)

Done! Pipelines will be updated to the new mode.

---

## File Structure

```
notebooks/
  ├── generate_pipelines_from_mock_api.py  ← Step 1: Configure & generate
  ├── deploy_pipelines.py                  ← Step 2: Deploy to workspace
  └── ingest_from_mock_api.py             ← Optional: One-time test

src/notebooks/
  └── pi_ingestion_pipeline.py            ← DLT pipeline code (auto-executed)

deployment/resources/
  ├── pipelines.yml                       ← Generated by Step 1
  └── jobs.yml                            ← Generated by Step 1

databricks.yml                            ← DAB configuration (no edits needed)
```

---

## Troubleshooting

### "Validation failed - notebook not found"
**Solution:** Check that `PROJECT_ROOT` in `deploy_pipelines.py` matches your repo location.

Common locations:
- `/Workspace/Repos/<username>/osipi-connector`
- `/Workspace/Users/<email>/osipi-connector`

### "Authentication failed"
**Solution:** Ensure you're running the notebooks in Databricks workspace (not locally).
The notebooks use workspace authentication automatically.

### "No data showing up"
**Solution:**
1. Check pipeline runs in DLT UI - are they succeeding?
2. Verify API endpoint is accessible: `MOCK_API_URL`
3. Check table permissions: `GRANT SELECT ON TABLE osipi.bronze.pi_timeseries TO <user>`

### "Too many/few pipelines"
**Solution:** Adjust `TAGS_PER_PIPELINE` in the generator notebook and re-run both notebooks.

---

## Advanced: Manual Pipeline Trigger

To manually trigger all pipelines after deployment, uncomment the code in the last cell of `deploy_pipelines.py`:

```python
# Uncomment to manually trigger all pipeline runs after deployment
# (see cell 8 in deploy_pipelines.py)
```

This is useful for testing or forcing a full refresh.

---

## Support

For issues or questions:
1. Check the troubleshooting section above
2. Review pipeline logs in DLT UI
3. Check job run logs (batch mode)
4. Contact your Databricks administrator
